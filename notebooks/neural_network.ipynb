{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "7d9827aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from src.activations import ActivationType, get_activation\n",
    "from src.initialiaztion import get_initialization\n",
    "from src.losses import get_loss\n",
    "from src.losses import LossType\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "ab7d6884",
   "metadata": {},
   "outputs": [],
   "source": [
    "xavier_initialization = get_initialization(\"xavier\")\n",
    "he_initialization = get_initialization(\"he\")\n",
    "\n",
    "relu = get_activation(\"relu\")[0]\n",
    "sigmoid = get_activation(\"sigmoid\")[0]\n",
    "softmax = get_activation(\"softmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "47220ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "  def __init__(self, \n",
    "               layer_dims: list[int], \n",
    "               activations: list[ActivationType], \n",
    "               loss_type=LossType, \n",
    "               optimizer_type : str = \"gd\",\n",
    "               seed : int = 42):\n",
    "    self._validate_inputs(layer_dims, activations, loss_type, optimizer_type)\n",
    "    \n",
    "    self.layer_dims = layer_dims\n",
    "    self.activations = activations\n",
    "    self.loss_type = loss_type\n",
    "    self.optimizer_type = optimizer_type\n",
    "    self.seed = seed\n",
    "    self.params = {}\n",
    "    \n",
    "    self._initialize_parameters()\n",
    "    \n",
    "  def forward_pass(self, X: np.ndarray):\n",
    "    # Validation\n",
    "    if self.layer_dims[0] != X.shape[0]:\n",
    "        raise ValueError(\n",
    "            f\"Input dimension mismatch. Expected {self.layer_dims[0]} features, \"\n",
    "            f\"but got {X.shape[0]}.\"\n",
    "        )\n",
    "\n",
    "    L = len(self.layer_dims)\n",
    "    A = X\n",
    "    caches = [] \n",
    "    \n",
    "    print(f\"\\n{'='*15} STARTING FORWARD PASS {'='*15}\")\n",
    "    print(f\"Input Batch Shape: {X.shape} ({X.shape[1]} examples)\")\n",
    "\n",
    "    for i in range(1, L):\n",
    "        act_name = self.activations[i - 1]\n",
    "        act_obj = get_activation(act_name)\n",
    "        \n",
    "        if isinstance(act_obj, tuple):\n",
    "            act_fnc = act_obj[0]\n",
    "        else:\n",
    "            act_fnc = act_obj\n",
    "\n",
    "        W = self.params[f\"W{i}\"]\n",
    "        b = self.params[f\"b{i}\"]\n",
    "        A_prev = A\n",
    "        \n",
    "        Z = np.dot(W, A_prev) + b \n",
    "        A = act_fnc(Z)\n",
    "        \n",
    "        print(f\"\\n--- Layer {i} ({act_name.upper()}) ---\")\n",
    "        print(f\"{'Input Matrix (A_prev)':<25} : {A_prev.shape}\")\n",
    "        print(f\"{'Weight Matrix (W)':<25} : {W.shape}\")\n",
    "        print(f\"{'Bias Vector (b)':<25} : {b.shape} (Broadcasts automatically)\")\n",
    "        print(f\"{'-'*45}\")\n",
    "        print(f\"{'Linear Step (Z = WA+b)':<25} : {Z.shape}\")\n",
    "        print(f\"{'Activation (A = f(Z))':<25} : {A.shape}\")\n",
    "        \n",
    "        caches.append((A_prev, Z))\n",
    "    \n",
    "    print(f\"\\n{'='*15} FORWARD PASS COMPLETE {'='*14}\\n\")\n",
    "    return A, caches\n",
    "\n",
    "  def backward_pass(self, y_true: np.ndarray, y_hat: np.ndarray, caches: list) -> dict:\n",
    "      if len(y_true) != len(y_hat):\n",
    "          raise ValueError(f\"y_true and y_hat must have the same shapes.\"\n",
    "                           f\"Got: y_true{y_true.shape}, y_hat{y_hat.shape}\")\n",
    "      \n",
    "      L = len(self.layer_dims)\n",
    "      m = y_true.shape[1]\n",
    "      \n",
    "      grads = {}\n",
    "      \n",
    "      #compute for last dA and dZ\n",
    "      A, Z = caches[L - 2]\n",
    "      \n",
    "      if self.loss_type in [\"bce\", \"cce\"]:\n",
    "          dZ = y_hat - y_true\n",
    "      else:\n",
    "          loss_derivative = get_loss(self.loss_type)[1]\n",
    "          dA = loss_derivative(y_hat, y_true)\n",
    "          \n",
    "          act_name = self.activations[-1]\n",
    "          act_obj = get_activation(act_name)\n",
    "          \n",
    "          if(isinstance(act_obj, tuple)):\n",
    "              act_derivative = act_obj[1]\n",
    "              dZ = dA * act_derivative(Z)\n",
    "          else:\n",
    "              dZ = dA\n",
    "              \n",
    "      grads[f\"dL_dW{L - 1}\"] = (1/m) * np.dot(dZ, A.T)\n",
    "      grads[f\"dL_db{L - 1}\"] = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "      \n",
    "      dA = np.dot(self.params[f\"W{L - 1}\"].T, dZ)\n",
    "      \n",
    "      for l in range( L - 2, 0, -1):\n",
    "          A, Z = caches[l - 1]\n",
    "          \n",
    "          act_obj = get_activation(self.activations[l - 1])\n",
    "      \n",
    "          if(isinstance(act_obj, tuple)):\n",
    "            act_derivative = act_obj[1]\n",
    "            dZ = act_derivative(Z) * dA\n",
    "          else:\n",
    "            raise ValueError(f\"Activation {act_name} has no derivative defined.\") \n",
    "          \n",
    "          grads[f\"dL_dW{l}\"] = (1/m) * np.dot(dZ, A.T)\n",
    "          grads[f\"dL_db{l}\"] = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "          \n",
    "          if(l > 1):\n",
    "            dA = np.dot(self.params[f\"W{l}\"].T, dZ)\n",
    "      \n",
    "      return grads\n",
    "      \n",
    "  def _initialize_parameters(self):\n",
    "      np.random.seed(self.seed)\n",
    "      \n",
    "      L = len(self.layer_dims)\n",
    "      \n",
    "      for i in range(1, L):\n",
    "          D_o =  self.layer_dims[i]\n",
    "          D_i = self.layer_dims[i - 1]\n",
    "          act_fnc = self.activations[i - 1]\n",
    "          \n",
    "          if act_fnc == \"relu\":\n",
    "              self.params[f\"W{i}\"] = he_initialization((D_o, D_i))\n",
    "          if act_fnc == \"sigmoid\" or act_fnc == \"softmax\" or act_fnc == \"linear\":\n",
    "              self.params[f\"W{i}\"] = xavier_initialization((D_o, D_i))\n",
    "          \n",
    "          self.params[f\"b{i}\"] = np.zeros((D_o, 1))\n",
    "          \n",
    "  def _validate_inputs(self, layer_dims, activations, loss_type, optimizer_type):\n",
    "    \"\"\"\n",
    "    Private helper to validate all inputs before initialization.\n",
    "    \"\"\"\n",
    "    if not isinstance(layer_dims, list):\n",
    "        raise TypeError(f\"layer_dims must be a list, got {type(layer_dims)}\")\n",
    "    \n",
    "    if not all(isinstance(x, int) for x in layer_dims):\n",
    "        raise TypeError(\"All elements in layer_dims must be integers!\")\n",
    "\n",
    "    if not isinstance(activations, list):\n",
    "          raise TypeError(f\"activations must be a list, got {type(activations)}\")\n",
    "\n",
    "    if len(layer_dims) < 2:\n",
    "        raise ValueError(\"The length of layers must be at least 2 (Input -> Output)\") \n",
    "    \n",
    "    if min(layer_dims) < 1:\n",
    "          raise ValueError(\"The number of neurons in every layer must be at least 1\")\n",
    "    \n",
    "    if len(layer_dims) != len(activations) + 1:\n",
    "          raise ValueError(\n",
    "              f\"Structure Error: You provided {len(layer_dims)} layers but {len(activations)} activations. \"\n",
    "              f\"Expected {len(layer_dims) - 1} activations.\"\n",
    "          )\n",
    "\n",
    "    valid_activations = {\"relu\", \"sigmoid\", \"softmax\", \"linear\"}\n",
    "    for act in activations:\n",
    "        if act not in valid_activations:\n",
    "            raise ValueError(f\"Invalid activation '{act}'. Supported: {valid_activations}\")\n",
    "\n",
    "    valid_losses = {\"mse\", \"bce\", \"cce\"}\n",
    "    if loss_type not in valid_losses:\n",
    "        raise ValueError(f\"Invalid loss_type '{loss_type}'. Supported: {valid_losses}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "a45327b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(\n",
    "  activations=[\"relu\", \"relu\", \"relu\", \"linear\"],\n",
    "  layer_dims=[3, 4, 2, 3, 2],\n",
    "  loss_type=\"mse\",\n",
    "  optimizer_type=\"adam\",\n",
    "  seed=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "49e955b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 0.04128346,  0.40820855, -0.81315624],\n",
       "        [ 0.56632081, -0.34154176, -1.29380189],\n",
       "        [-0.52885036,  0.48873458,  0.27128102],\n",
       "        [-0.93691075,  0.50514169, -0.07184103]]),\n",
       " 'b1': array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " 'W2': array([[ 0.30057157,  0.23493845, -0.81799262,  0.24819247],\n",
       "        [-0.42913411,  1.09387957,  0.51147976,  0.03262277]]),\n",
       " 'b2': array([[0.],\n",
       "        [0.]]),\n",
       " 'W3': array([[-0.98299165,  0.05443274],\n",
       "        [ 0.15989294, -1.20894816],\n",
       "        [ 2.22336022,  0.39429521]]),\n",
       " 'b3': array([[0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " 'W4': array([[ 0.97708318, -0.6424824 ,  0.94439928],\n",
       "        [-0.78575385, -0.37598541,  0.31318441]]),\n",
       " 'b4': array([[0.],\n",
       "        [0.]])}"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "30d188e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============== STARTING FORWARD PASS ===============\n",
      "Input Batch Shape: (3, 20) (20 examples)\n",
      "\n",
      "--- Layer 1 (RELU) ---\n",
      "Input Matrix (A_prev)     : (3, 20)\n",
      "Weight Matrix (W)         : (4, 3)\n",
      "Bias Vector (b)           : (4, 1) (Broadcasts automatically)\n",
      "---------------------------------------------\n",
      "Linear Step (Z = WA+b)    : (4, 20)\n",
      "Activation (A = f(Z))     : (4, 20)\n",
      "\n",
      "--- Layer 2 (RELU) ---\n",
      "Input Matrix (A_prev)     : (4, 20)\n",
      "Weight Matrix (W)         : (2, 4)\n",
      "Bias Vector (b)           : (2, 1) (Broadcasts automatically)\n",
      "---------------------------------------------\n",
      "Linear Step (Z = WA+b)    : (2, 20)\n",
      "Activation (A = f(Z))     : (2, 20)\n",
      "\n",
      "--- Layer 3 (RELU) ---\n",
      "Input Matrix (A_prev)     : (2, 20)\n",
      "Weight Matrix (W)         : (3, 2)\n",
      "Bias Vector (b)           : (3, 1) (Broadcasts automatically)\n",
      "---------------------------------------------\n",
      "Linear Step (Z = WA+b)    : (3, 20)\n",
      "Activation (A = f(Z))     : (3, 20)\n",
      "\n",
      "--- Layer 4 (LINEAR) ---\n",
      "Input Matrix (A_prev)     : (3, 20)\n",
      "Weight Matrix (W)         : (2, 3)\n",
      "Bias Vector (b)           : (2, 1) (Broadcasts automatically)\n",
      "---------------------------------------------\n",
      "Linear Step (Z = WA+b)    : (2, 20)\n",
      "Activation (A = f(Z))     : (2, 20)\n",
      "\n",
      "=============== FORWARD PASS COMPLETE ==============\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = np.random.normal(size=(3, 20))\n",
    "\n",
    "y_pred, caches = nn.forward_pass(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "7578e88b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04800625, -2.35807363, -1.10558404,  0.83783635,  2.08787087,\n",
       "         0.91484096, -0.27620335,  0.7965119 , -1.14379857,  0.50991978,\n",
       "        -1.3474603 , -0.0093601 , -0.13070464,  0.80208661, -0.30296397,\n",
       "         1.20200259, -0.19674528,  0.8365287 ,  0.78660228, -1.84087587],\n",
       "       [ 0.03754749,  0.03592805, -0.77873992,  0.17941071, -1.45553433,\n",
       "         0.55618522,  0.50977885,  0.30044554,  2.47658416,  0.3523434 ,\n",
       "         0.067471  , -0.7322647 ,  0.29714121, -0.9617768 ,  1.27181862,\n",
       "        -0.64764453,  0.15846954,  1.99008302,  1.16418756,  0.24266016],\n",
       "       [ 1.3799201 , -0.05455871,  0.79523395,  0.01908996, -0.90543814,\n",
       "         0.43027133,  0.93465006, -0.34610187, -1.09712188, -0.52819607,\n",
       "        -2.37977527, -0.60768369, -1.07529009,  2.02240507, -0.5648753 ,\n",
       "        -1.54292905,  0.87084178, -0.17521053,  0.04860301,  0.1886462 ]])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caches[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "a8435b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL_dW1:  (4, 3)\n",
      "W1:  (4, 3)\n",
      " \n",
      "dL_dW2:  (2, 4)\n",
      "W2:  (2, 4)\n",
      " \n",
      "dL_dW3:  (3, 2)\n",
      "W3:  (3, 2)\n",
      " \n",
      "dL_dW4:  (2, 3)\n",
      "W4:  (2, 3)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "y_true = y_pred - 0.2\n",
    "\n",
    "grads = nn.backward_pass(y_true, y_pred, caches)\n",
    "\n",
    "for i in range(1, 5):\n",
    "  print(f\"dL_dW{i}: \", grads[f\"dL_dW{i}\"].shape)\n",
    "  print(f\"W{i}: \", nn.params[f\"W{i}\"].shape)\n",
    "  print(\" \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
