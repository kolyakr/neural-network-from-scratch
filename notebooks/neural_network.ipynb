{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7d9827aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from src.activations import ActivationType, get_activation\n",
    "from src.initialiaztion import get_initialization\n",
    "from src.optimizers import OptimizerType, get_optimizer\n",
    "from src.losses import get_loss\n",
    "from src.losses import LossType\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ab7d6884",
   "metadata": {},
   "outputs": [],
   "source": [
    "xavier_initialization = get_initialization(\"xavier\")\n",
    "he_initialization = get_initialization(\"he\")\n",
    "\n",
    "relu = get_activation(\"relu\")[0]\n",
    "sigmoid = get_activation(\"sigmoid\")[0]\n",
    "softmax = get_activation(\"softmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "47220ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "  def __init__(self, \n",
    "               layer_dims: list[int], \n",
    "               activations: list[ActivationType], \n",
    "               loss_type=LossType, \n",
    "               optimizer_type : OptimizerType = \"gd\",\n",
    "               optimizer_params: dict = None,\n",
    "               seed : int = 42):\n",
    "    self._validate_inputs(layer_dims, activations, loss_type, optimizer_type)\n",
    "    \n",
    "    self.layer_dims = layer_dims\n",
    "    self.activations = activations\n",
    "    self.loss_type = loss_type\n",
    "    self.optimizer_type = optimizer_type\n",
    "    self.optimizer_params = optimizer_params if optimizer_params else {}\n",
    "    self.seed = seed\n",
    "    self.params = {}\n",
    "    \n",
    "    self._initialize_parameters()\n",
    "    self.optimizer = get_optimizer(self.optimizer_type, **self.optimizer_params)\n",
    "    \n",
    "    \n",
    "  def forward_pass(self, X: np.ndarray):\n",
    "    # Validation\n",
    "    if self.layer_dims[0] != X.shape[0]:\n",
    "        raise ValueError(\n",
    "            f\"Input dimension mismatch. Expected {self.layer_dims[0]} features, \"\n",
    "            f\"but got {X.shape[0]}.\"\n",
    "        )\n",
    "\n",
    "    L = len(self.layer_dims)\n",
    "    A = X\n",
    "    caches = [] \n",
    "    \n",
    "    for i in range(1, L):\n",
    "        act_name = self.activations[i - 1]\n",
    "        act_obj = get_activation(act_name)\n",
    "        \n",
    "        if isinstance(act_obj, tuple):\n",
    "            act_fnc = act_obj[0]\n",
    "        else:\n",
    "            act_fnc = act_obj\n",
    "\n",
    "        W = self.params[f\"W{i}\"]\n",
    "        b = self.params[f\"b{i}\"]\n",
    "        A_prev = A\n",
    "        \n",
    "        Z = np.dot(W, A_prev) + b \n",
    "        A = act_fnc(Z)\n",
    "        \n",
    "        caches.append((A_prev, Z))\n",
    "\n",
    "    return A, caches\n",
    "\n",
    "  def backward_pass(self, y_true: np.ndarray, y_hat: np.ndarray, caches: list) -> dict:\n",
    "      if len(y_true) != len(y_hat):\n",
    "          raise ValueError(f\"y_true and y_hat must have the same shapes.\"\n",
    "                           f\"Got: y_true{y_true.shape}, y_hat{y_hat.shape}\")\n",
    "      \n",
    "      L = len(self.layer_dims)\n",
    "      m = y_true.shape[1]\n",
    "      \n",
    "      grads = {}\n",
    "      loss = get_loss(self.loss_type)[0](y_hat, y_true)\n",
    "      \n",
    "      #compute for last dA and dZ\n",
    "      A, Z = caches[L - 2]\n",
    "      \n",
    "      if self.loss_type in [\"bce\", \"cce\"]:  \n",
    "          dZ = y_hat - y_true\n",
    "      else:\n",
    "          loss_derivative = get_loss(self.loss_type)[1]\n",
    "          dA = loss_derivative(y_hat, y_true)\n",
    "          \n",
    "          act_name = self.activations[-1]\n",
    "          act_obj = get_activation(act_name)\n",
    "          \n",
    "          if(isinstance(act_obj, tuple)):\n",
    "              act_derivative = act_obj[1]\n",
    "              dZ = dA * act_derivative(Z)\n",
    "          else:\n",
    "              dZ = dA\n",
    "              \n",
    "      grads[f\"dL_dW{L - 1}\"] = (1/m) * np.dot(dZ, A.T)\n",
    "      grads[f\"dL_db{L - 1}\"] = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "      \n",
    "      dA = np.dot(self.params[f\"W{L - 1}\"].T, dZ)\n",
    "      \n",
    "      for l in range( L - 2, 0, -1):\n",
    "          A, Z = caches[l - 1]\n",
    "          \n",
    "          act_obj = get_activation(self.activations[l - 1])\n",
    "      \n",
    "          if(isinstance(act_obj, tuple)):\n",
    "            act_derivative = act_obj[1]\n",
    "            dZ = act_derivative(Z) * dA\n",
    "          else:\n",
    "            raise ValueError(f\"Activation {act_name} has no derivative defined.\") \n",
    "          \n",
    "          grads[f\"dL_dW{l}\"] = (1/m) * np.dot(dZ, A.T)\n",
    "          grads[f\"dL_db{l}\"] = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "          \n",
    "          if(l > 1):\n",
    "            dA = np.dot(self.params[f\"W{l}\"].T, dZ)\n",
    "      \n",
    "      return grads, loss\n",
    "  \n",
    "  def train(self,\n",
    "          X_train: np.ndarray,\n",
    "          y_train: np.ndarray,\n",
    "          epochs: int = 1000,\n",
    "          learning_rate: float = 0.01,\n",
    "          batch_size: int = None,):\n",
    "  \n",
    "    if(X_train.shape[1] != y_train.shape[1]):\n",
    "        raise ValueError(f\"X_train and y_train must have the same size of observations.\"\n",
    "                        f\"Got:\"\n",
    "                        f\"X_train: {X_train.shape[1]}\"\n",
    "                        f\"y_train: {y_train.shape[1]}\")\n",
    "        \n",
    "    n = X_train.shape[1]\n",
    "        \n",
    "    if(batch_size != None and batch_size > n):\n",
    "        raise ValueError(f\"batch_size must not be bigger than the number of observations in dataset\"\n",
    "                        f\"Got:\"\n",
    "                        f\"X_train: {X_train.shape[1]}\"\n",
    "                        f\"batch_size: {batch_size}\")\n",
    "        \n",
    "    losses = []\n",
    "    idx = np.arange(0, n)\n",
    "    \n",
    "    for i in range(0, epochs):\n",
    "        \n",
    "        permutation = np.random.permutation(idx)\n",
    "        X_shuffled = X_train[:, permutation]\n",
    "        y_shuffled = y_train[:, permutation]\n",
    "        \n",
    "        for j in range(0, n, batch_size):\n",
    "        \n",
    "          y_hat, caches = self.forward_pass(X_shuffled[:, j: j + batch_size])\n",
    "          grads, batch_loss = self.backward_pass(y_shuffled[:, j: j + batch_size], y_hat, caches)\n",
    "          self.optimizer.update(\n",
    "              self.params,\n",
    "              grads,\n",
    "              learning_rate\n",
    "          )\n",
    "          \n",
    "          losses.append(batch_loss)\n",
    "    \n",
    "    return losses\n",
    "              \n",
    "  def _initialize_parameters(self):\n",
    "      np.random.seed(self.seed)\n",
    "      \n",
    "      L = len(self.layer_dims)\n",
    "      \n",
    "      for i in range(1, L):\n",
    "          D_o =  self.layer_dims[i]\n",
    "          D_i = self.layer_dims[i - 1]\n",
    "          act_fnc = self.activations[i - 1]\n",
    "          \n",
    "          if act_fnc == \"relu\":\n",
    "              self.params[f\"W{i}\"] = he_initialization((D_o, D_i))\n",
    "          if act_fnc == \"sigmoid\" or act_fnc == \"softmax\" or act_fnc == \"linear\":\n",
    "              self.params[f\"W{i}\"] = xavier_initialization((D_o, D_i))\n",
    "          \n",
    "          self.params[f\"b{i}\"] = np.zeros((D_o, 1))\n",
    "          \n",
    "  def _validate_inputs(self, layer_dims, activations, loss_type, optimizer_type):\n",
    "    \"\"\"\n",
    "    Private helper to validate all inputs before initialization.\n",
    "    \"\"\"\n",
    "    if not isinstance(layer_dims, list):\n",
    "        raise TypeError(f\"layer_dims must be a list, got {type(layer_dims)}\")\n",
    "    \n",
    "    if not all(isinstance(x, int) for x in layer_dims):\n",
    "        raise TypeError(\"All elements in layer_dims must be integers!\")\n",
    "\n",
    "    if not isinstance(activations, list):\n",
    "          raise TypeError(f\"activations must be a list, got {type(activations)}\")\n",
    "\n",
    "    if len(layer_dims) < 2:\n",
    "        raise ValueError(\"The length of layers must be at least 2 (Input -> Output)\") \n",
    "    \n",
    "    if min(layer_dims) < 1:\n",
    "          raise ValueError(\"The number of neurons in every layer must be at least 1\")\n",
    "    \n",
    "    if len(layer_dims) != len(activations) + 1:\n",
    "          raise ValueError(\n",
    "              f\"Structure Error: You provided {len(layer_dims)} layers but {len(activations)} activations. \"\n",
    "              f\"Expected {len(layer_dims) - 1} activations.\"\n",
    "          )\n",
    "\n",
    "    valid_activations = {\"relu\", \"sigmoid\", \"softmax\", \"linear\"}\n",
    "    for act in activations:\n",
    "        if act not in valid_activations:\n",
    "            raise ValueError(f\"Invalid activation '{act}'. Supported: {valid_activations}\")\n",
    "\n",
    "    valid_losses = {\"mse\", \"bce\", \"cce\"}\n",
    "    if loss_type not in valid_losses:\n",
    "        raise ValueError(f\"Invalid loss_type '{loss_type}'. Supported: {valid_losses}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a45327b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "def get_classification_data(n_samples=1000, noise=0.1, seed=42):\n",
    "    \"\"\"\n",
    "    Generates a binary classification dataset (Moons).\n",
    "    Returns shapes: X=(2, m), y=(1, m)\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Generate data using sklearn\n",
    "    # X_raw shape: (m, 2), y_raw shape: (m,)\n",
    "    X_raw, y_raw = make_moons(n_samples=n_samples, noise=noise, random_state=seed)\n",
    "    \n",
    "    # --- TRANSFORMATION FOR YOUR NN CLASS ---\n",
    "    # 1. Transpose X to get (features, samples)\n",
    "    X = X_raw.T \n",
    "    \n",
    "    # 2. Reshape y to (1, samples)\n",
    "    y = y_raw.reshape(1, -1)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def get_regression_data(n_samples=1000, seed=42):\n",
    "    \"\"\"\n",
    "    Generates a non-linear regression dataset (Noisy Sine Wave).\n",
    "    Returns shapes: X=(1, m), y=(1, m)\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Generate X values between -5 and 5\n",
    "    # Shape: (1, m)\n",
    "    X = np.random.uniform(-5, 5, (1, n_samples))\n",
    "    \n",
    "    # Generate y = sin(x) + Gaussian Noise\n",
    "    # Shape: (1, m)\n",
    "    noise = np.random.normal(0, 0.1, (1, n_samples))\n",
    "    y = np.sin(X) + noise\n",
    "    \n",
    "    # Normalize X to range [0, 1] or [-1, 1] usually helps NN convergence,\n",
    "    # but strictly for generation we leave it raw here.\n",
    "    # Note: Neural Nets struggle with unscaled data. \n",
    "    # It is recommended to scale X before training.\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "49e955b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_classification_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "30d188e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_nn = NeuralNetwork(\n",
    "  layer_dims=[2, 3, 3, 1],\n",
    "  activations=[\"relu\", \"relu\", \"sigmoid\"],\n",
    "  loss_type=\"bce\",\n",
    "  optimizer_type=\"adam\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7578e88b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 0.49671415, -0.1382643 ],\n",
       "        [ 0.64768854,  1.52302986],\n",
       "        [-0.23415337, -0.23413696]]),\n",
       " 'b1': array([[0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " 'W2': array([[ 1.28942186,  0.62660783, -0.38332423],\n",
       "        [ 0.44299842, -0.37837896, -0.38026675],\n",
       "        [ 0.19756137, -1.56218678, -1.40838951]]),\n",
       " 'b2': array([[0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " 'W3': array([[-0.32463686, -0.58475832,  0.18143078]]),\n",
       " 'b3': array([[0.]])}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_nn.params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f680ad56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_sklearn = X.T\n",
    "y_sklearn = y.T\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sklearn, y_sklearn, test_size=0.33, random_state=42)\n",
    "\n",
    "X_train = X_train.T\n",
    "X_test = X_test.T\n",
    "y_train = y_train.T\n",
    "y_test = y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d21e67f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = classification_nn.train(X_train,\n",
    "                                 y_train,\n",
    "                                 epochs=100,\n",
    "                                 learning_rate=0.01,\n",
    "                                 batch_size=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a8435b27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.03226919520137765)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9c09d479",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, caches = classification_nn.forward_pass(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ac9de5a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.20688741071401784)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bce = get_loss(\"bce\")[0]\n",
    "\n",
    "bce(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1187fc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[y_pred > 0.5] = 1\n",
    "y_pred[y_pred <= 0.5] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7fcbd795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b46ad8f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ff3c6621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.8939393939393939\n",
      "f1-score:  0.890282131661442\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "print(\"accuracy: \", accuracy_score(y_test.T, y_pred.T))\n",
    "print(\"f1-score: \", f1_score(y_test.T, y_pred.T))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
