{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38d1141b",
   "metadata": {},
   "source": [
    "**Parameter Initialization**\n",
    "\n",
    "Parameter initialization is a critical topic. While it might seem obvious to initialize weights randomly or set them to zero, doing so carelessly can break the model.\n",
    "\n",
    "*1. The Problem with Zero Initialization*\n",
    "\n",
    "It is acceptable to initialize biases ($b$) to zero. However, if we initialize Weights ($W$) to zero, we encounter the **Symmetry Problem**.\n",
    "* If all weights are zero, every neuron in the layer performs the same calculation and outputs the same value.\n",
    "* Consequently, during backpropagation, every neuron receives the same gradient update.\n",
    "* The neurons never \"break symmetry\" and fail to learn distinct features. The network effectively acts as a single linear neuron, no matter how deep it is.\n",
    "\n",
    "*2. The Problem with Random Initialization*\n",
    "\n",
    "If we simply pick random numbers without careful scaling, we run into gradient stability issues:\n",
    "\n",
    "* **Vanishing Gradient:** If weights are initialized too small, the signal shrinks as it passes through each layer. By the time backpropagation reaches the early layers, the gradients are close to zero. The network learns extremely slowly or stops entirely.\n",
    "    \n",
    "\n",
    "* **Exploding Gradient:** If weights are initialized too large, the signal grows uncontrollably with each layer. Gradients become massive, causing the weights to update wildly. This leads to unstable learning, divergence, or `NaN` (Not a Number) errors.\n",
    "\n",
    "*3. The Solution: Variance Control*\n",
    "\n",
    "To prevent these issues, we must control the **variance** of the weights. The goal is to keep the scale of the input signal roughly the same as the scale of the output signal across layers.\n",
    "\n",
    "*4. Implementation*\n",
    "\n",
    "To correctly initialize $W$, we look at the activation function and the number of input connections ($D_{in}$) from the previous layer.\n",
    "\n",
    "* **For ReLU (He Initialization):**\n",
    "    Formula: `np.random.randn(...) *` $\\sqrt{\\frac{2}{D_{in}}}$\n",
    "\n",
    "    *Reasoning:* Since ReLU zeros out negative values (killing half the signal), we double the variance to preserve the signal magnitude.\n",
    "\n",
    "* **For Sigmoid/Softmax (Xavier/Glorot Initialization):**\n",
    "    Formula: `np.random.randn(...) *` $\\sqrt{\\frac{1}{D_{in}}}$\n",
    "    \n",
    "    *Reasoning:* This keeps the signal variance within the \"linear\" middle region of the S-curve, preventing the gradients from vanishing at the flat tails of the function."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
