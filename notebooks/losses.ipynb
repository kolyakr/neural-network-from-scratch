{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaba71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5cc358",
   "metadata": {},
   "source": [
    "**The Loss Function: Maximum Likelihood**\n",
    "\n",
    "Okay, we set some parameters, train the model, and it works... works... **Are you sure?**\n",
    "\n",
    "In order to objectively estimate how well our model predicts the output, we need a function that fairly compares our predictions with the actual outputs. This is the **Loss Function**.\n",
    "\n",
    "We will start with the fundamental principle from which most other loss functions are derived: **Maximum Likelihood Estimation (MLE)**.\n",
    "\n",
    "$$\n",
    "\\hat{\\phi} = \\underset{\\phi}{\\arg\\max} \\prod_{i=1}^I P(y_i \\mid f(x_i, \\phi))\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "It is quite intuitive:\n",
    "\n",
    "* $f(x_i, \\phi)$: This is our model (function) with learnable parameters $\\phi$ acting on input $x_i$.\n",
    "* $P(y_i \\mid f(x_i, \\phi))$: This is the **likelihood** of observing the true target $y_i$. The model outputs parameters for a distribution (like the mean $\\mu$ for a Gaussian or the probability $\\lambda$ for a Bernoulli), and we check how probable the true data point $y_i$ is under that distribution.\n",
    "* **The Goal:** We want to **maximize the product** of these probabilities. If the product is high, it means our model parameters $\\phi$ make the observed data \"very likely\" to happen.\n",
    "\n",
    "> **Technical Note:**\n",
    "> While the formula above multiplies the probabilities ($\\prod$), in practice, multiplying many small numbers causes computer errors (underflow). Therefore, we usually take the **Negative Logarithm** of this formula.\n",
    ">\n",
    "> 1.  **Logarithm:** Turns the **Product** into a **Sum** (which is easier to calculate).\n",
    "> 2.  **Negative Sign:** In optimization, we prefer minimizing \"Loss\" rather than maximizing \"Likelihood.\"\n",
    ">\n",
    "> This gives us the **Negative Log-Likelihood (NLL)**:\n",
    "> $$\\hat{\\phi} = \\underset{\\phi}{\\arg\\min} \\left[ -\\sum_{i=1}^I \\log P(y_i \\mid f(x_i, \\phi)) \\right]$$\n",
    "\n",
    "We won't go through the full mathematical derivation for every function, but trust me that the principle above is fundamental.\n",
    "\n",
    "The main loss functions we will cover are:\n",
    "* **Least Squares Loss** (Mean Squared Error) – Derived from Gaussian distribution.\n",
    "* **Cross-Entropy Loss** – Derived from Bernoulli distribution (Binary classification).\n",
    "* **Multiclass Cross-Entropy Loss** – Derived from Categorical distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2417dd47",
   "metadata": {},
   "source": [
    "1. **Least squares loss**\n",
    "$$L = \\sum_{i=1}^N (y_i - \\hat{y}_i)^2$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06e55998",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_hat: np.ndarray, y_true: np.ndarray) -> float:\n",
    "  return np.sum((y_hat - y_true)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2f0800",
   "metadata": {},
   "source": [
    "**Derivative of MSE**\n",
    "\n",
    "Since the backward process (gradient computation) wants to discover how parameters $\\phi$ influence the total loss $L[\\phi]$, we must start at the end of the network.\n",
    "\n",
    "In order to obtain this, we start by computing the derivative of the loss with respect to our prediction function $\\hat{f}(x, \\phi)$ and move backward using the **Chain Rule**.\n",
    "\n",
    "\n",
    "\n",
    "Let's define the Mean Squared Error (MSE) for a batch of $I$ samples. Let $\\hat{y}_i = \\hat{f}(x_i, \\phi)$ be our prediction and $y_i$ be the true value.\n",
    "\n",
    "$$L = \\frac{1}{I} \\sum_{i=1}^I (\\hat{y}_i - y_i)^2$$\n",
    "\n",
    "We want to find $\\frac{\\partial L}{\\partial \\hat{y}_i}$ (how the Loss changes if the prediction changes). We apply the **Power Rule** of calculus ($\\frac{d}{dx}u^2 = 2u \\cdot u'$):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\hat{y}_i} = \\frac{\\partial}{\\partial \\hat{y}_i} \\left[ \\frac{1}{I} (\\hat{y}_i - y_i)^2 \\right]\n",
    "$$\n",
    "\n",
    "Since the sum involves other terms ($j \\neq i$) that are constants with respect to $\\hat{y}_i$, they disappear, leaving only the term for the current sample:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\hat{y}_i} = \\frac{1}{I} \\cdot 2 \\cdot (\\hat{y}_i - y_i) \\cdot \\frac{\\partial}{\\partial \\hat{y}_i}(\\hat{y}_i - y_i)\n",
    "$$\n",
    "\n",
    "Since the derivative of $(\\hat{y}_i - y_i)$ is just $1$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\hat{y}_i} = \\frac{2}{I} (\\hat{y}_i - y_i)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "321d5898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_derivative(y_hat, y_true):\n",
    "  return 2 * (y_hat - y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3647719",
   "metadata": {},
   "source": [
    "2. **Binary cross-entropy**\n",
    "$$L[\\phi] = - \\sum_i \\left[ y_i \\log(\\text{sig}(f[x_i, \\phi])) + (1 - y_i) \\log(1 - \\text{sig}(f[x_i, \\phi])) \\right]$$\n",
    "\n",
    "Here we use $sig(z)$(sigmoid) since as we assume that we have Bernoulli distribution, so we need parameter $\\lambda$, which is a probaility, and since our model will not return number from 0 to 1 - we pass it through sigmoid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf4ca1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z: np.ndarray) -> np.ndarray:\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4353b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(y_true, y_hat):\n",
    "  p = sigmoid(y_hat)\n",
    "  \n",
    "  epsilon = 1e-15\n",
    "  p = np.clip(p, epsilon, 1 - epsilon)\n",
    "  \n",
    "  return -np.mean(y_true * np.log(p) + (1 - y_true) * (np.log(1 - p)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2f1982",
   "metadata": {},
   "source": [
    "**Derivative of binary cross-entropy**\n",
    "\n",
    "Let’s define our variables:\n",
    "* **$z$**: The raw input (logit) from the linear layer ($z = wx + b$).\n",
    "* **$\\hat{y}$**: The predicted probability, calculated using the Sigmoid function:\n",
    "    $$\\hat{y} = \\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "* **$y$**: The actual ground truth label ($0$ or $1$).\n",
    "* **$L$**: The Binary Cross-Entropy Loss function:\n",
    "    $$L = -[y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y})]$$\n",
    "\n",
    "We want to find the gradient of the Loss with respect to the input $z$.\n",
    "$$\\frac{\\partial L}{\\partial z} = \\text{?}$$\n",
    "\n",
    "Since $L$ depends on $\\hat{y}$, and $\\hat{y}$ depends on $z$, we apply the Chain Rule:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial z} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z}$$\n",
    "\n",
    "We need to calculate these two parts separately.\n",
    "\n",
    "We take the derivative of the BCE formula:\n",
    "\n",
    "$$L = -y \\ln(\\hat{y}) - (1 - y) \\ln(1 - \\hat{y})$$\n",
    "\n",
    "Using the rule $\\frac{d}{dx}\\ln(x) = \\frac{1}{x}$:\n",
    "\n",
    "1.  Derivative of first term: $\\frac{-y}{\\hat{y}}$\n",
    "2.  Derivative of second term: $-(1-y) \\cdot \\frac{1}{1-\\hat{y}} \\cdot (-1)$ (Chain rule applied to $1-\\hat{y}$) $\\rightarrow \\frac{1-y}{1-\\hat{y}}$\n",
    "\n",
    "Combine them:\n",
    "$$\\frac{\\partial L}{\\partial \\hat{y}} = -\\frac{y}{\\hat{y}} + \\frac{1 - y}{1 - \\hat{y}}$$\n",
    "\n",
    "**Simplify by finding a common denominator:**\n",
    "$$= \\frac{-y(1 - \\hat{y}) + \\hat{y}(1 - y)}{\\hat{y}(1 - \\hat{y})}$$\n",
    "$$= \\frac{-y + y\\hat{y} + \\hat{y} - y\\hat{y}}{\\hat{y}(1 - \\hat{y})}$$\n",
    "$$= \\frac{\\hat{y} - y}{\\hat{y}(1 - \\hat{y})}$$\n",
    "\n",
    "This is just the derivative of the Sigmoid function, which we derived earlier.\n",
    "\n",
    "$$\\frac{\\partial \\hat{y}}{\\partial z} = \\hat{y}(1 - \\hat{y})$$\n",
    "\n",
    "Now we multiply Part A and Part B. Watch the magic happen:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z} = \\underbrace{\\left( \\frac{\\hat{y} - y}{\\hat{y}(1 - \\hat{y})} \\right)}_{\\text{Part A}} \\cdot \\underbrace{\\left( \\hat{y}(1 - \\hat{y}) \\right)}_{\\text{Part B}}\n",
    "$$\n",
    "\n",
    "The denominator of the Loss derivative **perfectly cancels out** the derivative of the Sigmoid function.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z} = \\hat{y} - y\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad6c0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy_derivative(y_true, y_hat):\n",
    "  return sigmoid(y_hat) - y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59b8cd3",
   "metadata": {},
   "source": [
    "3. **Categorical cross-entropy**\n",
    "\n",
    "The **Multiclass Cross-Entropy Loss** (also known as Categorical Cross-Entropy) is the standard loss function used in machine learning for classification tasks with more than two classes.\n",
    "\n",
    "It quantifies the difference between the **true probability distribution** (typically represented as one-hot encoded labels) and the **predicted probability distribution** output by the model. Conceptually, it encourages the model to assign a high probability to the correct class and suppresses the probabilities of incorrect classes.\n",
    "\n",
    "For a batch of $N$ samples and $K$ classes, the loss $L$ is defined as:\n",
    "$$L = -\\frac{1}{N} \\sum_{i=1}^N \\sum_{k=1}^K y_{i,k} \\log(p_{i,k})$$\n",
    "**Where:**\n",
    "- $N$: The batch size (number of samples).\n",
    "- $K$: The number of classes.\n",
    "- $y_{i,k}$: The true label for sample $i$ and class $k$.\n",
    "    - $y_{i,k} = 1$ if $k$ is the correct class.\n",
    "    - $y_{i,k} = 0$ otherwise (One-Hot Encoding).\n",
    "- $p_{i,k}$: The predicted probability that sample $i$ belongs to class $k$.\n",
    "\n",
    "\n",
    "**The Softmax Connection**\n",
    "\n",
    "The predicted probability $p_{i,k}$ is usually computed via the **Softmax function** applied to the model's raw logits (scores) $f$:\n",
    "$$p_{i,k} = \\text{softmax}(f_k) = \\frac{e^{f_k[x_i, \\phi]}}{\\sum_{k'=1}^K e^{f_{k'}[x_i, \\phi]}}$$\n",
    "\n",
    "Because the true labels $y_{i,k}$ are **one-hot encoded** (only one $1$, the rest are $0$), the inner summation collapses. We only care about the probability of the _correct_ class ($y_i$):\n",
    "$$L = -\\frac{1}{N} \\sum_{i=1}^N \\log(p_{i, y_i})$$\n",
    "\n",
    "\n",
    "In practice (e.g., PyTorch's `nn.CrossEntropyLoss`), we substitute the Softmax equation directly into the Loss equation. This is the **Log-Sum-Exp** form, which is numerically stable:\n",
    "\n",
    "$$L = -\\frac{1}{N} \\sum_{i=1}^N \\left( \\underbrace{f_{y_i}[x_i, \\phi]}_{\\text{Score of Correct Class}} - \\underbrace{\\log \\sum_{k'=1}^K e^{f_{k'}[x_i, \\phi]}}_{\\text{LogSumExp (Total Energy)}} \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e396ba60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_cross_entropy(y_true, y_hat):\n",
    "  \n",
    "  loss = 0\n",
    "  n = len(y_true)\n",
    "  \n",
    "  for true_class_idx, logits in zip(y_true, y_hat):\n",
    "    log_sum_exp = np.log(np.sum(np.exp(logits)))\n",
    "    \n",
    "    loss += logits[true_class_idx] - log_sum_exp\n",
    "    \n",
    "  return -(loss / n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151e2b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_cross_entropy_derivative(y_true, y_hat):\n",
    "  return sigmoid(y_hat) - y_true"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
